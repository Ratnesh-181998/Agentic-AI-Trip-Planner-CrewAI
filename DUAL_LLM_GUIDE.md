# ğŸ¤– Dual LLM System - Groq + Ollama

## Overview

Your AI Trip Planner now supports **two LLM providers** with intelligent automatic fallback:

1. **Groq** (Cloud-based) - âš¡ Fast (30-90 seconds)
2. **Ollama** (Local) - ğŸŒ Slower (5-15 minutes) but offline

---

## ğŸ¯ How It Works

### **Auto Mode (Default - Recommended)**
```
Try Groq â†’ If fails â†’ Switch to Ollama
```

**When Groq fails:**
- âŒ API key invalid/missing
- âŒ Rate limit exceeded
- âŒ Quota exhausted
- âŒ Network issues
- âŒ Service unavailable

**Automatic fallback to Ollama ensures:**
- âœ… Your app always works
- âœ… No manual intervention needed
- âœ… Seamless user experience

---

## ğŸ›ï¸ Three Modes Available

### 1. **Auto (Groq â†’ Ollama)** â­ Recommended
- Tries Groq first (fast)
- Falls back to Ollama if Groq fails
- Best of both worlds

### 2. **Groq Only**
- Only uses Groq API
- Fastest option
- Requires valid API key
- Fails if quota exhausted

### 3. **Ollama Only**
- Only uses local Ollama
- Works offline
- Slower but reliable
- Requires Ollama running locally

---

## ğŸ“‹ Setup Instructions

### Option 1: Use Groq (Fast - Recommended)

1. **Get Free API Key**
   - Visit: https://console.groq.com/keys
   - Sign up (free)
   - Create API key

2. **Set API Key**
   
   **Method A: Environment Variable** (Recommended)
   ```powershell
   # Windows PowerShell
   $env:GROQ_API_KEY="gsk_your_key_here"
   ```
   
   **Method B: Direct in Code**
   - Edit `TravelAgents.py` line 16
   - Replace with your API key

3. **Run App**
   ```bash
   streamlit run my_app_2.py
   ```

### Option 2: Use Ollama (Offline)

1. **Install Ollama**
   - Download: https://ollama.ai/download
   - Install for your OS

2. **Pull Model**
   ```bash
   ollama pull llama3.2
   ```

3. **Start Ollama Server**
   ```bash
   ollama serve
   ```

4. **Run App**
   ```bash
   streamlit run my_app_2.py
   ```

### Option 3: Use Both (Auto Fallback) â­

1. Follow both setup instructions above
2. The app will automatically use Groq first
3. Falls back to Ollama if Groq fails

---

## ğŸ¨ UI Controls

In the Streamlit sidebar, you can:

1. **Choose LLM Provider**
   - Auto (Groq â†’ Ollama)
   - Groq Only
   - Ollama Only

2. **View Speed Comparison**
   - Groq: ~30-90 seconds
   - Ollama: ~5-15 minutes

3. **See Current Status**
   - Active provider
   - Warnings/info messages

---

## ğŸ”§ Technical Details

### File: `TravelAgents.py`

```python
def get_llm(force_groq=False, force_ollama=False):
    """
    Smart LLM selector with automatic fallback
    
    Args:
        force_groq: Only use Groq (no fallback)
        force_ollama: Only use Ollama (skip Groq)
    
    Returns:
        LLM instance (Groq or Ollama)
    """
```

### Fallback Logic

```
1. Check force_ollama flag
   â”œâ”€ Yes â†’ Use Ollama
   â””â”€ No â†’ Continue

2. Try Groq initialization
   â”œâ”€ Success â†’ Return Groq LLM
   â””â”€ Failure â†’ Continue

3. Check force_groq flag
   â”œâ”€ Yes â†’ Raise error (no fallback)
   â””â”€ No â†’ Continue

4. Try Ollama initialization
   â”œâ”€ Success â†’ Return Ollama LLM
   â””â”€ Failure â†’ Return Groq (last resort)
```

---

## ğŸ“Š Performance Comparison

| Feature | Groq | Ollama |
|---------|------|--------|
| **Speed** | âš¡ 30-90 sec | ğŸŒ 5-15 min |
| **Cost** | ğŸ†“ Free (with limits) | ğŸ†“ Free |
| **Internet** | âœ… Required | âŒ Not required |
| **Setup** | Easy (API key) | Medium (install) |
| **Reliability** | High (99.9% uptime) | Very High (local) |
| **Privacy** | Cloud-based | 100% Local |

---

## ğŸ†˜ Troubleshooting

### Groq Issues

**Error: "Invalid API Key"**
```bash
# Check your API key
echo $env:GROQ_API_KEY  # Windows PowerShell
```

**Error: "Rate Limit Exceeded"**
- Wait a few minutes
- App will auto-switch to Ollama
- Or manually select "Ollama Only"

**Error: "Quota Exhausted"**
- Groq free tier: 14,400 requests/day
- App will auto-switch to Ollama
- Or wait 24 hours for reset

### Ollama Issues

**Error: "Connection refused"**
```bash
# Make sure Ollama is running
ollama serve
```

**Error: "Model not found"**
```bash
# Pull the model
ollama pull llama3.2
```

**Error: "Ollama not installed"**
- Download from: https://ollama.ai/download
- Install and restart terminal

---

## ğŸ¯ Best Practices

### For Development
- Use **Auto mode** for best experience
- Groq for speed, Ollama as backup

### For Production
- Use **Groq Only** with monitoring
- Set up quota alerts
- Have Ollama ready as backup

### For Offline Use
- Use **Ollama Only**
- Pre-pull required models
- Ensure sufficient RAM (8GB+)

---

## ğŸ“ˆ Monitoring

### Check Active LLM

The console will show:
```
ğŸš€ Attempting to use Groq LLM (fast cloud-based)...
âœ… Groq LLM initialized successfully!
```

Or:
```
âš ï¸ Groq LLM failed: Rate limit exceeded
ğŸ”„ Falling back to local Ollama LLM...
âœ… Ollama LLM initialized successfully!
```

---

## ğŸ” Security Notes

### Groq API Key
- Never commit API keys to Git
- Use environment variables
- Rotate keys regularly
- Monitor usage at: https://console.groq.com/

### Ollama
- Runs locally (no data sent to cloud)
- Full privacy control
- No API key needed

---

## ğŸ“ Support

- **Groq**: https://console.groq.com/docs
- **Ollama**: https://ollama.ai/docs
- **CrewAI**: https://docs.crewai.com/

---

## âœ… Summary

Your app now has:
- âœ… Dual LLM support (Groq + Ollama)
- âœ… Automatic fallback system
- âœ… UI controls for manual selection
- âœ… Detailed logging and status
- âœ… Error handling and recovery
- âœ… Works online and offline

**Result**: A robust, fast, and reliable AI Trip Planner! ğŸš€
